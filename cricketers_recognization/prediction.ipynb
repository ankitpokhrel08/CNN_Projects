{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88e0e1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2809d333",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "1. Splitting image data into train and test sets (80/20 ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cd2573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: players/train\n",
      "Created directory: players/test\n"
     ]
    }
   ],
   "source": [
    "# # Define the directories\n",
    "# SOURCE_DIR = \"players/images\"\n",
    "# TRAIN_DIR = \"players/train\"\n",
    "# TEST_DIR = \"players/test\"\n",
    "\n",
    "# # Create train and test directories if they don't exist\n",
    "# for directory in [TRAIN_DIR, TEST_DIR]:\n",
    "#     if not os.path.exists(directory):\n",
    "#         os.makedirs(directory)\n",
    "#         print(f\"Created directory: {directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295123c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to split images for a player\n",
    "# def split_player_images(player_folder, train_ratio=0.8):\n",
    "#     \"\"\"\n",
    "#     Split images from a player's folder into train and test sets\n",
    "    \n",
    "#     Args:\n",
    "#         player_folder: Path to the player's folder\n",
    "#         train_ratio: Ratio of images to use for training (default: 0.8)\n",
    "#     \"\"\"\n",
    "#     player_name = os.path.basename(player_folder)\n",
    "    \n",
    "#     # Create player directories in train and test if they don't exist\n",
    "#     train_player_dir = os.path.join(TRAIN_DIR, player_name)\n",
    "#     test_player_dir = os.path.join(TEST_DIR, player_name)\n",
    "    \n",
    "#     for directory in [train_player_dir, test_player_dir]:\n",
    "#         if not os.path.exists(directory):\n",
    "#             os.makedirs(directory)\n",
    "    \n",
    "#     # Get all image files\n",
    "#     image_files = [f for f in os.listdir(player_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.webp', '.gif'))]\n",
    "    \n",
    "#     if not image_files:\n",
    "#         print(f\"No images found for player: {player_name}\")\n",
    "#         return 0, 0\n",
    "    \n",
    "#     # Shuffle the images to ensure random split\n",
    "#     random.shuffle(image_files)\n",
    "    \n",
    "#     # Calculate split index\n",
    "#     split_idx = int(len(image_files) * train_ratio)\n",
    "    \n",
    "#     # Split into train and test sets\n",
    "#     train_images = image_files[:split_idx]\n",
    "#     test_images = image_files[split_idx:]\n",
    "    \n",
    "#     # Copy images to train directory\n",
    "#     for img in train_images:\n",
    "#         src = os.path.join(player_folder, img)\n",
    "#         dst = os.path.join(train_player_dir, img)\n",
    "#         shutil.copy2(src, dst)\n",
    "    \n",
    "#     # Copy images to test directory\n",
    "#     for img in test_images:\n",
    "#         src = os.path.join(player_folder, img)\n",
    "#         dst = os.path.join(test_player_dir, img)\n",
    "#         shutil.copy2(src, dst)\n",
    "    \n",
    "#     return len(train_images), len(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf53675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split all player folders\n",
    "# def process_all_players(source_dir, train_ratio=0.8):\n",
    "#     \"\"\"\n",
    "#     Process all player folders and split their images\n",
    "#     \"\"\"\n",
    "#     if not os.path.exists(source_dir):\n",
    "#         print(f\"Source directory '{source_dir}' does not exist!\")\n",
    "#         return\n",
    "    \n",
    "#     player_folders = [os.path.join(source_dir, d) for d in os.listdir(source_dir) \n",
    "#                       if os.path.isdir(os.path.join(source_dir, d))]\n",
    "    \n",
    "#     if not player_folders:\n",
    "#         print(f\"No player folders found in '{source_dir}'\")\n",
    "#         return\n",
    "    \n",
    "#     print(f\"Found {len(player_folders)} player folders\")\n",
    "    \n",
    "#     results = {}\n",
    "#     total_train = 0\n",
    "#     total_test = 0\n",
    "    \n",
    "#     for folder in player_folders:\n",
    "#         player_name = os.path.basename(folder)\n",
    "#         print(f\"Processing {player_name}...\", end=\" \")\n",
    "        \n",
    "#         train_count, test_count = split_player_images(folder, train_ratio)\n",
    "        \n",
    "#         results[player_name] = {\"train\": train_count, \"test\": test_count}\n",
    "#         total_train += train_count\n",
    "#         total_test += test_count\n",
    "        \n",
    "#         print(f\"Split {train_count} train, {test_count} test images\")\n",
    "    \n",
    "#     print(\"\\nSummary:\")\n",
    "#     print(f\"Total training images: {total_train}\")\n",
    "#     print(f\"Total testing images: {total_test}\")\n",
    "#     print(f\"Total images: {total_train + total_test}\")\n",
    "#     print(f\"Train/Test ratio: {total_train/(total_train + total_test):.2f}/{total_test/(total_train + total_test):.2f}\")\n",
    "    \n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d00fa50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 92 player folders\n",
      "Processing thisara_perera... Split 8 train, 3 test images\n",
      "Processing adil_rashid... Split 12 train, 4 test images\n",
      "Processing jason_behrendorff... Split 16 train, 4 test images\n",
      "Processing aaron_finch... Split 8 train, 2 test images\n",
      "Processing fakhar_zaman... Split 8 train, 2 test images\n",
      "Processing mohammad_hafeez... Split 12 train, 4 test images\n",
      "Processing jp_duminy... Split 11 train, 3 test images\n",
      "Processing vijay_shankar... Split 13 train, 4 test images\n",
      "Processing angelo_mathews... Split 16 train, 4 test images\n",
      "Processing sarfaraz_ahmed... Split 9 train, 3 test images\n",
      "Processing kane_williamson... Split 14 train, 4 test images\n",
      "Processing mushfiqur_rahim... Split 6 train, 2 test images\n",
      "Processing adam_zampa... Split 15 train, 4 test images\n",
      "Processing lasith_malinga... Split 12 train, 3 test images\n",
      "Processing gyanendra_malla... Split 8 train, 3 test images\n",
      "Processing rohit_sharma... Split 8 train, 3 test images\n",
      "Processing pratis_gc... Split 7 train, 2 test images\n",
      "Processing paras_khadka... Split 9 train, 3 test images\n",
      "Processing ben_stokes... Split 10 train, 3 test images\n",
      "Processing tom_curran... Split 12 train, 3 test images\n",
      "Processing mujeeb_ur_rahman... Split 11 train, 3 test images\n",
      "Processing avishka_fernando... Split 10 train, 3 test images\n",
      "Processing mitchell_starc... Split 14 train, 4 test images\n",
      "Processing dinesh_karthik... Split 15 train, 4 test images\n",
      "Processing gulsan_jha... Split 8 train, 3 test images\n",
      "Processing bhim_sharki... Split 8 train, 3 test images\n",
      "Processing moeen_ali... Split 16 train, 4 test images\n",
      "Processing ross_taylor... Split 13 train, 4 test images\n",
      "Processing anrich_nortje... Split 16 train, 4 test images\n",
      "Processing david_warner... Split 12 train, 4 test images\n",
      "Processing marcus_stoinis... Split 12 train, 4 test images\n",
      "Processing chris_morris... Split 6 train, 2 test images\n",
      "Processing shadab_khan... Split 8 train, 3 test images\n",
      "Processing jason_roy... Split 12 train, 3 test images\n",
      "Processing virat_kohli... Split 14 train, 4 test images\n",
      "Processing shakib_al_hasan... Split 15 train, 4 test images\n",
      "Processing andre_russell... Split 13 train, 4 test images\n",
      "Processing basanta_regmi... Split 6 train, 2 test images\n",
      "Processing imam-ul-haq... Split 12 train, 3 test images\n",
      "Processing ab_de_villiers... Split 12 train, 3 test images\n",
      "Processing bibek_yadav... Split 5 train, 2 test images\n",
      "Processing lungi_ngidi... Split 9 train, 3 test images\n",
      "Processing avinash_bohara... Split 7 train, 2 test images\n",
      "Processing jonny_bairstow... Split 8 train, 2 test images\n",
      "Processing dale_steyn... Split 16 train, 5 test images\n",
      "Processing lalit_rajbanshi... Split 6 train, 2 test images\n",
      "Processing joe_root... Split 11 train, 3 test images\n",
      "Processing rijan_dhakal... Split 7 train, 2 test images\n",
      "Processing nicholas_pooran... Split 15 train, 4 test images\n",
      "Processing gulsan_jha... Split 8 train, 3 test images\n",
      "Processing bhim_sharki... Split 8 train, 3 test images\n",
      "Processing moeen_ali... Split 16 train, 4 test images\n",
      "Processing ross_taylor... Split 13 train, 4 test images\n",
      "Processing anrich_nortje... Split 16 train, 4 test images\n",
      "Processing david_warner... Split 12 train, 4 test images\n",
      "Processing marcus_stoinis... Split 12 train, 4 test images\n",
      "Processing chris_morris... Split 6 train, 2 test images\n",
      "Processing shadab_khan... Split 8 train, 3 test images\n",
      "Processing jason_roy... Split 12 train, 3 test images\n",
      "Processing virat_kohli... Split 14 train, 4 test images\n",
      "Processing shakib_al_hasan... Split 15 train, 4 test images\n",
      "Processing andre_russell... Split 13 train, 4 test images\n",
      "Processing basanta_regmi... Split 6 train, 2 test images\n",
      "Processing imam-ul-haq... Split 12 train, 3 test images\n",
      "Processing ab_de_villiers... Split 12 train, 3 test images\n",
      "Processing bibek_yadav... Split 5 train, 2 test images\n",
      "Processing lungi_ngidi... Split 9 train, 3 test images\n",
      "Processing avinash_bohara... Split 7 train, 2 test images\n",
      "Processing jonny_bairstow... Split 8 train, 2 test images\n",
      "Processing dale_steyn... Split 16 train, 5 test images\n",
      "Processing lalit_rajbanshi... Split 6 train, 2 test images\n",
      "Processing joe_root... Split 11 train, 3 test images\n",
      "Processing rijan_dhakal... Split 7 train, 2 test images\n",
      "Processing nicholas_pooran... Split 4 train, 2 test images\n",
      "Processing kuldeep_yadav... Split 16 train, 4 test images\n",
      "Processing shikhar_dhawan... Split 16 train, 4 test images\n",
      "Processing rashid_khan... Split 12 train, 3 test images\n",
      "Processing jos_buttler... Split 9 train, 3 test images\n",
      "Processing karan_kc... Split 7 train, 2 test images\n",
      "Processing kemar_roach... Split 15 train, 4 test images\n",
      "Processing jasprit_bumrah... Split 15 train, 4 test images\n",
      "Processing tabraiz_shamsi... Split 12 train, 3 test images\n",
      "Processing shimron_hetmyer... Split 11 train, 3 test images\n",
      "Processing kushal_bhurtel... Split 8 train, 3 test images\n",
      "Processing eoin_morgan... Split 14 train, 4 test images\n",
      "Processing anil_sah... Split 7 train, 2 test images\n",
      "Processing dipendra_airee... Split 10 train, 3 test images\n",
      "Processing aasif_sheikh... Split 6 train, 2 test images\n",
      "Processing nandan_yadav... Split 6 train, 2 test images\n",
      "Processing aakash_chand... Split 5 train, 2 test images\n",
      "Processing ms_dhoni... Split 11 train, 3 test images\n",
      "Processing rohit_paudel... Split 12 train, 4 test images\n",
      "Processing quinton_de_kock... Split 8 train, 3 test images\n",
      "Processing jeevan_mendis... Split 8 train, 2 test images\n",
      "Processing jason_holder... Split 9 train, 3 test images\n",
      "Processing steve_smith... Split 9 train, 3 test images\n",
      "Processing bhuvneshwar_kumar... Split 16 train, 4 test images\n",
      "Processing dev_khanal... Split 7 train, 2 test images\n",
      "Processing kushal_malla... Split 4 train, 2 test images\n",
      "Processing kuldeep_yadav... Split 16 train, 4 test images\n",
      "Processing shikhar_dhawan... Split 16 train, 4 test images\n",
      "Processing rashid_khan... Split 12 train, 3 test images\n",
      "Processing jos_buttler... Split 9 train, 3 test images\n",
      "Processing karan_kc... Split 7 train, 2 test images\n",
      "Processing kemar_roach... Split 15 train, 4 test images\n",
      "Processing jasprit_bumrah... Split 15 train, 4 test images\n",
      "Processing tabraiz_shamsi... Split 12 train, 3 test images\n",
      "Processing shimron_hetmyer... Split 11 train, 3 test images\n",
      "Processing kushal_bhurtel... Split 8 train, 3 test images\n",
      "Processing eoin_morgan... Split 14 train, 4 test images\n",
      "Processing anil_sah... Split 7 train, 2 test images\n",
      "Processing dipendra_airee... Split 10 train, 3 test images\n",
      "Processing aasif_sheikh... Split 6 train, 2 test images\n",
      "Processing nandan_yadav... Split 6 train, 2 test images\n",
      "Processing aakash_chand... Split 5 train, 2 test images\n",
      "Processing ms_dhoni... Split 11 train, 3 test images\n",
      "Processing rohit_paudel... Split 12 train, 4 test images\n",
      "Processing quinton_de_kock... Split 8 train, 3 test images\n",
      "Processing jeevan_mendis... Split 8 train, 2 test images\n",
      "Processing jason_holder... Split 9 train, 3 test images\n",
      "Processing steve_smith... Split 9 train, 3 test images\n",
      "Processing bhuvneshwar_kumar... Split 16 train, 4 test images\n",
      "Processing dev_khanal... Split 7 train, 2 test images\n",
      "Processing kushal_malla... Split 8 train, 2 test images\n",
      "Processing kusal_perera... Split 12 train, 3 test images\n",
      "Processing lokesh_bam... Split 5 train, 2 test images\n",
      "Processing samiullah_shinwari... Split 8 train, 2 test images\n",
      "Processing gulbadin_naib... Split 14 train, 4 test images\n",
      "Processing pat_cummins... Split 12 train, 4 test images\n",
      "Processing isuru_udana... Split 12 train, 3 test images\n",
      "Processing ravindra_jadeja... Split 13 train, 4 test images\n",
      "Processing babar_azam... Split 9 train, 3 test images\n",
      "Processing hardik_pandya... Split 9 train, 3 test images\n",
      "Processing sharad_vesawkar... Split 10 train, 3 test images\n",
      "Processing shoaib_malik... Split 8 train, 3 test images\n",
      "Processing faf_du_plessis... Split 8 train, 2 test images\n",
      "Processing trent_boult... Split 11 train, 3 test images\n",
      "Processing alex_carey... Split 9 train, 3 test images\n",
      "Processing nathan_coulter-nile... Split 14 train, 4 test images\n",
      "Processing sandeep_lamichhane... Split 11 train, 3 test images\n",
      "Processing k._l._rahul... Split 16 train, 4 test images\n",
      "Processing sompal_kami... Split 9 train, 3 test images\n",
      "\n",
      "Summary:\n",
      "Total training images: 967\n",
      "Total testing images: 284\n",
      "Total images: 1251\n",
      "Train/Test ratio: 0.77/0.23\n",
      "Split 8 train, 2 test images\n",
      "Processing kusal_perera... Split 12 train, 3 test images\n",
      "Processing lokesh_bam... Split 5 train, 2 test images\n",
      "Processing samiullah_shinwari... Split 8 train, 2 test images\n",
      "Processing gulbadin_naib... Split 14 train, 4 test images\n",
      "Processing pat_cummins... Split 12 train, 4 test images\n",
      "Processing isuru_udana... Split 12 train, 3 test images\n",
      "Processing ravindra_jadeja... Split 13 train, 4 test images\n",
      "Processing babar_azam... Split 9 train, 3 test images\n",
      "Processing hardik_pandya... Split 9 train, 3 test images\n",
      "Processing sharad_vesawkar... Split 10 train, 3 test images\n",
      "Processing shoaib_malik... Split 8 train, 3 test images\n",
      "Processing faf_du_plessis... Split 8 train, 2 test images\n",
      "Processing trent_boult... Split 11 train, 3 test images\n",
      "Processing alex_carey... Split 9 train, 3 test images\n",
      "Processing nathan_coulter-nile... Split 14 train, 4 test images\n",
      "Processing sandeep_lamichhane... Split 11 train, 3 test images\n",
      "Processing k._l._rahul... Split 16 train, 4 test images\n",
      "Processing sompal_kami... Split 9 train, 3 test images\n",
      "\n",
      "Summary:\n",
      "Total training images: 967\n",
      "Total testing images: 284\n",
      "Total images: 1251\n",
      "Train/Test ratio: 0.77/0.23\n"
     ]
    }
   ],
   "source": [
    "# # Run the splitting process\n",
    "# split_results = process_all_players(SOURCE_DIR, train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097e05e0",
   "metadata": {},
   "source": [
    "`I just renamed my test folder to validation.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db22470",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "2. Performing data augmentation to enhance the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bbc06091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aakash_chand', 'aasif_sheikh', 'ab_de_villiers', 'anil_sah', 'avinash_bohara', 'basanta_regmi', 'bhim_sharki', 'bibek_yadav', 'dev_khanal', 'dipendra_airee', 'gulsan_jha', 'gyanendra_malla', 'karan_kc', 'kushal_bhurtel', 'kushal_malla', 'lalit_rajbanshi', 'lokesh_bam', 'nandan_yadav', 'paras_khadka', 'pratis_gc', 'rijan_dhakal', 'rohit_paudel', 'sandeep_lamichhane', 'sharad_vesawkar', 'sompal_kami', 'virat_kohli']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "categories = os.listdir('players/train')\n",
    "categories.sort()\n",
    "print(categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fd090a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c7de192a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 208 images belonging to 26 classes.\n",
      "Found 66 images belonging to 26 classes.\n"
     ]
    }
   ],
   "source": [
    "#Data Augmentation\n",
    "batch_size = 16 #This is the number of images to be processed in a batch\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'players/train', \n",
    "        target_size=(224, 224),  \n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  \n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'players/validation', \n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "68f5bca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting gyanendra_malla: 100%|██████████| 8/8 [00:03<00:00,  2.52it/s]\n",
      "Augmenting pratis_gc: 100%|██████████| 7/7 [00:02<00:00,  2.64it/s]\n",
      "Augmenting paras_khadka: 100%|██████████| 9/9 [00:03<00:00,  2.59it/s]\n",
      "Augmenting gulsan_jha: 100%|██████████| 8/8 [00:03<00:00,  2.49it/s]\n",
      "Augmenting bhim_sharki: 100%|██████████| 8/8 [00:03<00:00,  2.60it/s]\n",
      "Augmenting virat_kohli: 100%|██████████| 14/14 [00:05<00:00,  2.60it/s]\n",
      "Augmenting basanta_regmi: 100%|██████████| 6/6 [00:02<00:00,  2.45it/s]\n",
      "Augmenting ab_de_villiers: 100%|██████████| 12/12 [00:04<00:00,  2.50it/s]\n",
      "Augmenting bibek_yadav: 100%|██████████| 5/5 [00:01<00:00,  2.57it/s]\n",
      "Augmenting avinash_bohara: 100%|██████████| 7/7 [00:02<00:00,  2.42it/s]\n",
      "Augmenting lalit_rajbanshi: 100%|██████████| 6/6 [00:02<00:00,  2.56it/s]\n",
      "Augmenting rijan_dhakal: 100%|██████████| 7/7 [00:02<00:00,  2.50it/s]\n",
      "Augmenting karan_kc: 100%|██████████| 7/7 [00:02<00:00,  2.60it/s]\n",
      "Augmenting kushal_bhurtel: 100%|██████████| 8/8 [00:03<00:00,  2.54it/s]\n",
      "Augmenting anil_sah: 100%|██████████| 7/7 [00:02<00:00,  2.59it/s]\n",
      "Augmenting dipendra_airee: 100%|██████████| 10/10 [00:03<00:00,  2.58it/s]\n",
      "Augmenting aasif_sheikh: 100%|██████████| 6/6 [00:02<00:00,  2.58it/s]\n",
      "Augmenting nandan_yadav: 100%|██████████| 6/6 [00:02<00:00,  2.56it/s]\n",
      "Augmenting aakash_chand: 100%|██████████| 5/5 [00:01<00:00,  2.54it/s]\n",
      "Augmenting rohit_paudel: 100%|██████████| 12/12 [00:04<00:00,  2.53it/s]\n",
      "Augmenting dev_khanal: 100%|██████████| 7/7 [00:02<00:00,  2.53it/s]\n",
      "Augmenting kushal_malla: 100%|██████████| 8/8 [00:03<00:00,  2.57it/s]\n",
      "Augmenting lokesh_bam: 100%|██████████| 5/5 [00:01<00:00,  2.50it/s]\n",
      "Augmenting sharad_vesawkar: 100%|██████████| 10/10 [00:03<00:00,  2.63it/s]\n",
      "Augmenting sandeep_lamichhane: 100%|██████████| 11/11 [00:04<00:00,  2.57it/s]\n",
      "Augmenting sompal_kami: 100%|██████████| 9/9 [00:03<00:00,  2.59it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Original training data directory (with class subfolders)\n",
    "train_dir = 'players/train'\n",
    "\n",
    "# Define augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# How many augmented images to create per original image\n",
    "aug_per_image = 100\n",
    "\n",
    "for class_name in os.listdir(train_dir):\n",
    "    class_path = os.path.join(train_dir, class_name)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue  # Skip non-directory files\n",
    "\n",
    "    for img_name in tqdm(os.listdir(class_path), desc=f\"Augmenting {class_name}\"):\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "\n",
    "        try:\n",
    "            img = load_img(img_path, target_size=(224, 224))\n",
    "            x = img_to_array(img)\n",
    "            x = x.reshape((1,) + x.shape)\n",
    "\n",
    "            i = 0\n",
    "            for batch in datagen.flow(x, batch_size=1,\n",
    "                                      save_to_dir=class_path,\n",
    "                                      save_prefix='aug',\n",
    "                                      save_format='jpeg'):\n",
    "                i += 1\n",
    "                if i >= aug_per_image:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ffa0b81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting gyanendra_malla: 100%|██████████| 3/3 [00:00<00:00, 21.21it/s]\n",
      "Augmenting pratis_gc: 100%|██████████| 2/2 [00:00<00:00, 24.07it/s]\n",
      "Augmenting paras_khadka: 100%|██████████| 3/3 [00:00<00:00, 25.09it/s]\n",
      "Augmenting gulsan_jha: 100%|██████████| 3/3 [00:00<00:00, 24.77it/s]\n",
      "Augmenting bhim_sharki: 100%|██████████| 3/3 [00:00<00:00, 25.03it/s]\n",
      "Augmenting virat_kohli: 100%|██████████| 4/4 [00:00<00:00, 20.81it/s]\n",
      "Augmenting basanta_regmi: 100%|██████████| 2/2 [00:00<00:00, 23.98it/s]\n",
      "Augmenting ab_de_villiers: 100%|██████████| 3/3 [00:00<00:00, 23.64it/s]\n",
      "Augmenting bibek_yadav: 100%|██████████| 2/2 [00:00<00:00, 25.26it/s]\n",
      "Augmenting avinash_bohara: 100%|██████████| 2/2 [00:00<00:00, 23.56it/s]\n",
      "Augmenting lalit_rajbanshi: 100%|██████████| 2/2 [00:00<00:00, 24.73it/s]\n",
      "Augmenting rijan_dhakal: 100%|██████████| 2/2 [00:00<00:00, 22.60it/s]\n",
      "Augmenting karan_kc: 100%|██████████| 2/2 [00:00<00:00, 24.54it/s]\n",
      "Augmenting kushal_bhurtel: 100%|██████████| 3/3 [00:00<00:00, 25.18it/s]\n",
      "Augmenting dipendra_airee 1-19-33-232 PM: 100%|██████████| 3/3 [00:00<00:00, 24.32it/s]\n",
      "Augmenting anil_sah: 100%|██████████| 2/2 [00:00<00:00, 25.30it/s]\n",
      "Augmenting aasif_sheikh: 100%|██████████| 2/2 [00:00<00:00, 25.43it/s]\n",
      "Augmenting nandan_yadav: 100%|██████████| 2/2 [00:00<00:00, 25.15it/s]\n",
      "Augmenting aakash_chand: 100%|██████████| 2/2 [00:00<00:00, 24.83it/s]\n",
      "Augmenting rohit_paudel: 100%|██████████| 4/4 [00:00<00:00, 23.64it/s]\n",
      "Augmenting dev_khanal: 100%|██████████| 2/2 [00:00<00:00, 23.34it/s]\n",
      "Augmenting kushal_malla: 100%|██████████| 2/2 [00:00<00:00, 24.80it/s]\n",
      "Augmenting lokesh_bam: 100%|██████████| 2/2 [00:00<00:00, 24.63it/s]\n",
      "Augmenting sharad_vesawkar: 100%|██████████| 3/3 [00:00<00:00, 25.14it/s]\n",
      "Augmenting sandeep_lamichhane: 100%|██████████| 3/3 [00:00<00:00, 24.71it/s]\n",
      "Augmenting sompal_kami: 100%|██████████| 3/3 [00:00<00:00, 24.54it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Original training data directory (with class subfolders)\n",
    "train_dir = 'players/validation'\n",
    "\n",
    "# Define augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# How many augmented images to create per original image\n",
    "aug_per_image = 10\n",
    "\n",
    "for class_name in os.listdir(train_dir):\n",
    "    class_path = os.path.join(train_dir, class_name)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue  # Skip non-directory files\n",
    "\n",
    "    for img_name in tqdm(os.listdir(class_path), desc=f\"Augmenting {class_name}\"):\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "\n",
    "        try:\n",
    "            img = load_img(img_path, target_size=(224, 224))\n",
    "            x = img_to_array(img)\n",
    "            x = x.reshape((1,) + x.shape)\n",
    "\n",
    "            i = 0\n",
    "            for batch in datagen.flow(x, batch_size=1,\n",
    "                                      save_to_dir=class_path,\n",
    "                                      save_prefix='aug',\n",
    "                                      save_format='jpeg'):\n",
    "                i += 1\n",
    "                if i >= aug_per_image:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663a682a",
   "metadata": {},
   "source": [
    "# Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "032456e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20173 files belonging to 26 classes.\n",
      "Found 725 files belonging to 26 classes.\n"
     ]
    }
   ],
   "source": [
    "#Generators in Keras where we work on batches to process images\n",
    "train_ds = keras.utils.image_dataset_from_directory(\n",
    "    directory = 'players/train',\n",
    "    labels='inferred',\n",
    "    label_mode = 'categorical',\n",
    "    batch_size=32,\n",
    "    image_size=(224,224),\n",
    ")\n",
    "\n",
    "validation_ds = keras.utils.image_dataset_from_directory(\n",
    "    directory = 'players/validation',\n",
    "    labels='inferred',\n",
    "    label_mode = 'categorical',\n",
    "    batch_size=32,\n",
    "    image_size=(224,224),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9d456d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3deb5885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize so that we don't have 0-255 but 0-1\n",
    "def process(image, label):\n",
    "  image = tf.cast(image/255. ,tf.float32)\n",
    "  return image, label\n",
    "\n",
    "train_ds = train_ds.map(process)\n",
    "validation_ds = validation_ds.map(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fdeb48a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n",
    "#Import batchnormalization and dropout from keras\n",
    "from keras.layers import BatchNormalization, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8b73b41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ankitpokhrel/Downloads/All projects/ML_Projects/cricketers_recognization/myenv/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_12\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_12\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_28          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_29          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_30          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100352</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,845,184</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,354</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_30 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_28          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_30 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_31 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_29          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_31 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_32 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_30          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_32 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_12 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100352\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_43 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │    \u001b[38;5;34m12,845,184\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_44 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m)             │         \u001b[38;5;34m3,354\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,942,682</span> (49.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,942,682\u001b[0m (49.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,942,234</span> (49.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,942,234\u001b[0m (49.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create CNN model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#Convolution Layer-1\n",
    "model.add(Conv2D(32, kernel_size=(3,3), padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "#Pooling layer\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=2, padding='valid'))\n",
    "\n",
    "#Convolution Layer-2\n",
    "model.add(Conv2D(64, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "#Normalization Layer\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "#Pooling layer\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=2, padding='valid'))\n",
    "\n",
    "#Convolution Layer-3\n",
    "model.add(Conv2D(128, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "#Normalization Layer\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "#Pooling layer\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=2, padding='valid'))\n",
    "\n",
    "\n",
    "\n",
    "#Flattening\n",
    "model.add(Flatten())\n",
    "\n",
    "#Full-Connected ANN\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "\n",
    "#Dropout layers\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(Dense(units=26, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "339cd040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Check if the labeles are correctly mapped\n",
    "# print(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "69051eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9798fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aakash_chand': 0, 'aasif_sheikh': 1, 'ab_de_villiers': 2, 'anil_sah': 3, 'avinash_bohara': 4, 'basanta_regmi': 5, 'bhim_sharki': 6, 'bibek_yadav': 7, 'dev_khanal': 8, 'dipendra_airee': 9, 'gulsan_jha': 10, 'gyanendra_malla': 11, 'karan_kc': 12, 'kushal_bhurtel': 13, 'kushal_malla': 14, 'lalit_rajbanshi': 15, 'lokesh_bam': 16, 'nandan_yadav': 17, 'paras_khadka': 18, 'pratis_gc': 19, 'rijan_dhakal': 20, 'rohit_paudel': 21, 'sandeep_lamichhane': 22, 'sharad_vesawkar': 23, 'sompal_kami': 24, 'virat_kohli': 25}\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "# print(train_generator.class_indices)\n",
    "# print(train_generator.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "489fa852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1138s\u001b[0m 2s/step - accuracy: 0.1077 - loss: 5.1962 - val_accuracy: 0.0566 - val_loss: 4.2403\n",
      "Epoch 2/30\n",
      "\u001b[1m216/631\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m12:11\u001b[0m 2s/step - accuracy: 0.1081 - loss: 3.1264"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[101]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#This will burn your PC try at your own risk\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_ds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All projects/ML_Projects/cricketers_recognization/myenv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All projects/ML_Projects/cricketers_recognization/myenv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All projects/ML_Projects/cricketers_recognization/myenv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All projects/ML_Projects/cricketers_recognization/myenv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All projects/ML_Projects/cricketers_recognization/myenv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All projects/ML_Projects/cricketers_recognization/myenv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All projects/ML_Projects/cricketers_recognization/myenv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All projects/ML_Projects/cricketers_recognization/myenv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All projects/ML_Projects/cricketers_recognization/myenv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All projects/ML_Projects/cricketers_recognization/myenv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All projects/ML_Projects/cricketers_recognization/myenv/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All projects/ML_Projects/cricketers_recognization/myenv/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#This will burn your PC try at your own risk\n",
    "history = model.fit(train_ds, epochs=30, validation_data=validation_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d01cf4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d4b07db",
   "metadata": {},
   "source": [
    "## Overfitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0c8ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 394ms/step - accuracy: 0.2144 - loss: 2.9503 - val_accuracy: 0.0607 - val_loss: 3.5329\n",
      "Epoch 2/10\n",
      "\u001b[1m 46/631\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:21\u001b[0m 447ms/step - accuracy: 0.7160 - loss: 1.2015"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[96]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     22\u001b[39m model.compile(optimizer=Adam(learning_rate=\u001b[32m0.0001\u001b[39m),\n\u001b[32m     23\u001b[39m               loss=\u001b[33m'\u001b[39m\u001b[33mcategorical_crossentropy\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     24\u001b[39m               metrics=[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_ds\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All projects/ML_Projects/cricketers_recognization/myenv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All projects/ML_Projects/cricketers_recognization/myenv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All projects/ML_Projects/cricketers_recognization/myenv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All projects/ML_Projects/cricketers_recognization/myenv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All projects/ML_Projects/cricketers_recognization/myenv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All projects/ML_Projects/cricketers_recognization/myenv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All projects/ML_Projects/cricketers_recognization/myenv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All projects/ML_Projects/cricketers_recognization/myenv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All projects/ML_Projects/cricketers_recognization/myenv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All projects/ML_Projects/cricketers_recognization/myenv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All projects/ML_Projects/cricketers_recognization/myenv/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All projects/ML_Projects/cricketers_recognization/myenv/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.applications import MobileNetV2\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# # Load pre-trained model\n",
    "# base_model = MobileNetV2(include_top=False, input_shape=(224, 224, 3), weights='imagenet')\n",
    "# base_model.trainable = False  # freeze base layers\n",
    "\n",
    "# # Add custom top layers with anti-overfitting components\n",
    "# x = base_model.output\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Dense(128, activation='relu')(x)\n",
    "# x = Dropout(0.5)(x)  # Drop 50% of neurons to prevent overfitting\n",
    "# x = BatchNormalization()(x)\n",
    "# output = Dense(26, activation='softmax')(x)\n",
    "\n",
    "# model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# # Compile\n",
    "# model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Train\n",
    "# model.fit(\n",
    "#     train_ds,\n",
    "#     epochs=10,\n",
    "#     validation_data=validation_ds\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b60f5f1",
   "metadata": {},
   "source": [
    "### Final Verdict\n",
    "The model when trained with augmented data, the accuracy was not increasing significantly because the data required for model to work with augmentation is insufficient. If you want to use this model then try to collect as many data as you can around 10-20k images per class. The model will work!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6bf5ee",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
